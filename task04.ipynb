{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": " \nhttps://shimo.im/docs/dewFJi6mB48KAiPy/read\n任务：\n\n## (1)从基础概率推导贝叶斯公式，朴素贝叶斯公式\n1、 条件概率\n\n$P(AB)\u003dP(B)P(A|B)$\n$P(A|B)\u003dP(AB)/P(B)$\n\n2、乘法公式\n\n$P(AB)\u003dP(A|B)P(B)\u003dP(B|A)P(A)$\n\n推广：\n\n$P(A_1A_2...A_{n-1}A_{n})\u003dP(A_1)P(A_2|A_1)P(A_3|A_2A_1)...P(A_n|A_1A_2...A_{n-1}A_n)$\n\n3、全概率公式：\n\n 1. 如果事件组B1，B2，.... 满足\n \n 1.B1，B2....两两互斥，即 Bi ∩ Bj \u003d ∅ ，i≠j ， i,j\u003d1，2，....，且P(Bi)\u003e0,i\u003d1,2,....;\n \n 2.B1∪B2∪....\u003dΩ ，则称事件组 B1,B2,...是样本空间Ω的一个划分\n \n设 B1,B2,...是样本空间Ω的一个划分，A为任一事件，则：\n\n2.全概率公式的意义在于，\n        \n\u003e当直接计算P(A)较为困难,而P(Bi),P(A|Bi)  (i\u003d1,2,...)的计算较为简单时，可以利用全概率公式计算P(A)。思想就是，将事件A分解成几个小事件，通过求小事件的概率，然后相加从而求得事件A的概率，而将事件A进行分割的时候，不是直接对A进行分割，而是先找到样本空间Ω的一个个划分B1,B2,...Bn,这样事件A就被事件AB1,AB2,...ABn分解成了n部分，即A\u003dAB1+AB2+...+ABn, 每一Bi发生都可能导致A发生相应的概率是P(A|Bi)，由加法公式得\n\n$P(A)\u003d\\sum_{i\u003d1}^{\\infty}P(B_i)P(A|B_i)$\n\n $P(A)\u003dP(AB_1)+P(AB_2)+....+P(AB_n)$\n \n$\u003dP(A|B_1)P(B_1)+P(A|B_2)P(B_2)+...+P(A|B_n)P(PB_n)$\n\n### 贝叶斯公式\n\n在条件概率和全概率的基础上，很容易推导出贝叶斯公式：\n\n$P(A|B)\u003d\\frac{P(A) \\cap(B)}{P(B)}\u003d\\frac{P(B|A) \\times P(A)}{P(B)}\u003d\\frac{P(B|A) \\times P(A)}{P(B|A) \\times P(A)+P(B|A^c) \\times P(A^c)}$\n\n$ P(A^c)\u003d1- P(A)$\n\n## (2)先验概率（prior probility）\n\n 指根据以往经验和分析。在实验或采样前就可以得到的概率\n\n\n## (3)学习后验概率(posterior probility)\n\n指某件事已经发生，想要计算这件事发生的原因是由某个因素引起的概率。\n\n\u003e可以看出，先验概率就是事先可估计的概率分布，而后验概率类似贝叶斯公式“由果溯因”的思想。下面我们通过PRML（Pattern Recognition and Machine Learning）这本书中的例子来理解一下上面的定义。\n\n\u003e假设我们现在有两个盒子，分别为红色和蓝色。在红色盒子中放着2个苹果和6个橙子，在蓝色盒子中放着1个橙子和3个苹果，如下图所示：\n\n![avatar](https://raw.githubusercontent.com/xumajie/datawhale/master/lihongyi-ML/task04/posterior_01.jpg)\n\n\u003e图中绿色表示苹果，橙色代表橙子。假设我们每次实验的时候会随机从某个盒子里挑出一个水果，随机变量B（box）表示挑出的是哪个盒子，并且P(B\u003dblue) \u003d 0.6（蓝色盒子被选中的概率），P(B\u003dred) \u003d 0.4（红色盒子被选中的概率）。随机变量F（fruit）表示挑中的是哪种水果，F的取值为\"a (apple)\"和\"o (orange)\"。\n\n\u003e现在假设我们已经得知某次实验中挑出的水果是orange，那么这个orange是从红色盒子里挑出的概率是多大呢？依据贝叶斯公式有：\n\n$P(B\u003dred|F\u003do)\u003d\\frac{P(F\u003do|B\u003dred)P(B\u003dred)}{P(F\u003do)}\u003d\\frac{3}{4}\\times\\frac{4}{10}\\times\\frac{20}{9}\u003d\\frac{2}{3}$\n\n\u003e同时，由概率的加法规则我们可以得到：\n\n$P(B\u003dblue|F\u003do)\u003d1-\\frac{2}{3}\u003d\\frac{1}{3} $\n\u003e在上面的计算过程中，我们将 P(B\u003dred) 或者说 P(B) 称为先验概率（prior probability），因为我们在得到F是“a”或者“o”之前，就可以得到 P(B) 。同理，将 P(B\u003dred|F\u003do) 和 P(B\u003dblue|F\u003do) 称为后验概率，因为我们在完整的一次实验之后也就是得到了F的具体取值之后才能得到这个概率。\n## (4)学习LR和linear regreeesion之间的区别\n\n- 线性回归和逻辑回归都是广义线性回归模型的特例\n- 线性回归只能用于回归问题，逻辑回归用于分类问题（可由二分类推广至多分类）\n- 线性回归无联系函数或不起作用，逻辑回归的联系函数是对数几率函数，属于Sigmoid函数\n- 线性回归使用最小二乘法作为参数估计方法，逻辑回归使用极大似然法作为参数估计方法\n\nhttps://zhuanlan.zhihu.com/p/39363869\n## 推导sigmoid function公式(5)\n\n$\\sigma(x)\u003d\\frac{1}{1+e^{-x}}$\n\n二分类问题【二项式分布】\n\n$ P(y|x)\u003d\\{\\begin{matrix}p,y\u003d1\\\\ 1-p,y\u003d0\\end{matrix}$\n\n\n等价于：\n\n$P(y_i|x_i)\u003dp^{y_i}(1-p)^{(1-y^i)}$\n\n\u003e解释下这个函数的含义，我们采集到了一个样本 $(x_i,y_i)$ 。对这个样本，它的标签是 $y_i$ 的概率是 $p^{y_i}(1-p)^{1-{y_i}}$ 。 （当y\u003d1，结果是p；当y\u003d0，结果是1-p）。\n\n\u003e如果我们采集到了一组数据一共N个， $\\{(x_1,y_1),(x_2,y_2),(x_3,y_3)...(x_N,y_N)\\}$ ，这个合成在一起的合事件发生的总概率怎么求呢？其实就是将每一个样本发生的概率相乘就可以了，即采集到这组样本的概率：\n\n使用联合概率求损失函数，交叉熵函数\n$P_{总}\u003dP(y_1|x_1)P(y_2|x_2)...P(y_N|x_N)\u003d \\prod_{n\u003d1}^{N} p^{y_n}(1-p)^{1-y_n}$\n\n注意P_{总 } 是一个函数，并且未知的量只有 $w$ （在p里面）。\n\n\n由于连乘很复杂，我们通过两边取对数来把连乘变成连加的形式，即：\n\n$L(w)\u003dln(P_{总})\u003dln(\\prod_{n\u003d1}^{N} p^{y_n}(1-p)^{1-y_n})$\n\n$\u003d\\sum_{n\u003d1}^N ln(p^{y_n}(1-p)^{1-y_n})$\n\n$\u003d\\sum_{n\u003d1}^N (y_n\\ln (p) + (1-y_n) \\ln (1-p)))$\n\n其中：$p\u003d\\frac{1}{1+e^{-w^Tx}}$\n\n这个函数 $L(w)$ 又叫做它的损失函数。损失函数可以理解成衡量我们当前的模型的输出结果，跟实际的输出结果之间的差距的一种函数。这里的损失函数的值等于事件发生的总概率，我们希望它越大越好。但是跟损失的含义有点儿违背，因此也可以在前面取个负号。\n\n- 梯度推到过程\n\n$p\u003d\\frac{1}{1+e^{-w^Tx}}$求导：\n\n$p\u0027\u003d(\\frac{1}{1+e^{-w^Tx}})\u0027\u003d(\\frac{1}{(1+e^{-w^Tx})})\u0027(e^{-w^Tx})\u0027(-w^Tx)\u0027$\n\n$\u003d\\frac{1}{(1+e^{-w^Tx})^2} e^{-w^Tx} x$\n\n这步很关键：\n\n$\u003d\\frac{e^{-w^Tx}}{(1+e^{-w^Tx})} \\frac{1}{(1+e^{-w^Tx})} x$\n\n$\u003d\\frac{1}{(1+e^{-w^Tx})} \\frac{e^{-w^Tx}}{(1+e^{-w^Tx})}  x$\n\n$\u003d\\frac{1}{(1+e^{-w^Tx})} (1-\\frac{1}{(1+e^{-w^Tx})}) x$\n\n代入：$p\u003d\\frac{1}{1+e^{-w^Tx}}$\n\n$p\u0027\u003dp(1-p)x$\n\n$(1-p)\u0027\u003d-p\u0027\u003d-p(1-p)x$\n\n$\\nabla L(w)\u003d\\nabla (\\sum_{n\u003d1}^N (y_n\\ln (p) + (1-y_n) \\ln (1-p))))$\n\n$\u003d(\\sum (y_n\\ln (p) + (1-y_n) \\ln (1-p))))\u0027$\n\n$\u003d\\sum (y_n\\frac{1}{p}p\u0027 + (1-y_n)\\frac{1}{1-p} (1-p)\u0027)$\n\n$\u003d\\sum (y_n(1-p)x_n + (1-y_n)(-p)x_n)$\n\n$\u003d\\sum_{n\u003d1}^N (y_n-p)x_n$\n\n代入：$p\u003d\\frac{1}{1+e^{-w^Tx}}$\n\n$\u003d\\sum_{n\u003d1}^N (y_n-\\frac{1}{1+e^{-w^Tx}})x_n$\n\n\u003e在SGD中，我们每次只要均匀地、随机选取其中一个样本 $(x_i,y_i)$ ,用它代表整体样本，即把它的值乘以N，就相当于获得了梯度的无偏估计值，即 $E(G(w)) \u003d \\nabla L(w)$ ，因此SGD的更新公式为：\n\n$w_{t+1} \u003d w_t + \\eta  N  {(y_n- \\frac{1}{1+e^{-w^Tx_n}} )x_n}$\n\n这样我们前面的求和就没有了，同时 $\\eta$  N 都是常数， N 的值刚好可以并入 $\\eta$  当中,因此SGD的迭代更新公式为：\n\n$w_{t+1} \u003d w_t + \\eta   {(y_n- \\frac{1}{1+e^{-w^Tx_n}} )x_n}$\n\n其中 $(x_i,y_i) $是对所有样本随机抽样的一个结果。\n\n- 总结\n\n终于一切都搞清楚了，现在我们来理一理思路，首先逻辑回归模型长这样：\n\n$y\u003d\\frac{1}{1+e^{-w^Tx}}$\n\n其中我们不知道的量是 $w$ ，假设我们已经训练好了一个 $w^*$ , 我们用模型来判断 $x_i$ 的标签呢？很简单，直接将$x_i$代入y中，求出来的值就是x_i的标签是1的概率，如果概率大于0.5，那么我们认为它就是1类，否则就是0类。\n\n那怎么得到 $w^*$ 呢？\n\n如果采用随机梯度下降法的话，我们首先随机产生一个w的初始值 w_0 ,然后通过公式不断迭代从而求得w^*的值：\n\n$w_{t+1} \u003d w_t + \\eta   {(y_n- \\frac{1}{1+e^{-w^Tx_n}} )x_n}$\n\n每次迭代都从所有样本中随机抽取一个 $(x_i,y_i)$ 来代入上述方程。\n\n参考：https://zhuanlan.zhihu.com/p/44591359\n"
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}