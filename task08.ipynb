{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "## 信息增益\n\n信息增益(information gain)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。特征A训练数据集D的信息増益$g(D,A)$，定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D∣A)$之差，即\n\n$g(D,A)\u003dH(D)−H(D∣A)$\n\n## 信息增益率\n\n信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比(information gain ratio)可以对这一问题进行校正。这是特征选择的另一准则。\n\n信息增益比：特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D的经验熵$H(D)$之比\n\n## 1. ID3算法\n\n1) 定义：ID3算法的核心是在决策树各个结点上应用[信息增益]准则选择特征，递归地构建决策树。\n\n具体方法是：从根结点(root node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树。直到所有特征的信息增益均很小或没有特征可以选择为止，最后得到一个决策树。ID3 相当于用极大似然法进行概率模型的选择。\n\n2）优点\n\n- 理论清晰，算法简单，很有实用价值的示例学习算法；\n- 计算时间是例子个数、特征属性个数、节点个数之积的线性函数，总预测准确率较令人满意。\n3）缺点\n\n- 存在偏向问题，各特征属性的取值个数会影响互信息量的大小；\n- 特征属性间的相关性强调不够，是单变元算法；\n- 对噪声较为敏感，训练数据的轻微错误会导致结果的不同；\n- 结果随训练集记录个数的改变而不同，不便于进行渐进学习；\n- 生成的树容易产生过拟合。\n\n## 2. C4.5的生成算法\n1）定义\n\nC4.5算法与ID3算法相似，C4.5 算法对ID3算法进行了改进。C4.5 在生成的过程中，用信息增益比来选择特征。\n\n2）优点\n\n- 通过信息增益率选择分裂属性，克服了ID3算法中通过信息增益倾向于选择拥有多个属性值的属性作为分裂属性的不足；\n- 能够处理离散型和连续型的属性类型，即将连续型的属性进行离散化处理；\n- 构造决策树之后进行剪枝操作；\n- 能够处理具有缺失属性值的训练数据。\n\n3）缺点\n\n- C4.5生成的是多叉树，即一个父节点可以有多个节点，因此算法的计算效率较低，特别是针对含有连续属性值的训练样本时表现的尤为突出。\n- 算法在选择分裂属性时没有考虑到条件属性间的相关性，只计算数据集中每一个条件属性与决策属性之间的期望信息，有可能影响到属性选择的正确性。\n- C4.5算法只适合于能够驻留内存的数据集，当训练集大得无法在内存容纳时，程序无法运行；\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "data": {
            "text/plain": "{\u0027density\u0027: {\u00270.243\u0027: 0,\n  \u00270.245\u0027: 0,\n  \u00270.34299999999999997\u0027: 0,\n  \u00270.36\u0027: 0,\n  \u00270.40299999999999997\u0027: 1,\n  \u00270.43700000000000006\u0027: 1,\n  \u00270.48100000000000004\u0027: 1,\n  \u00270.556\u0027: 1,\n  \u00270.593\u0027: 0,\n  \u00270.608\u0027: 1,\n  \u00270.634\u0027: 1,\n  \u00270.639\u0027: 0,\n  \u00270.657\u0027: 0,\n  \u00270.6659999999999999\u0027: 0,\n  \u00270.6970000000000001\u0027: 1,\n  \u00270.7190000000000001\u0027: 0,\n  \u00270.774\u0027: 1}}"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 1
        }
      ],
      "source": "import pandas as pd\nimport numpy as np\ndata\u003dpd.read_csv(\u0027resource/watermelon_3a.csv\u0027)\n\ndef calc_entropy(dataSet):\n    m\u003dlen(dataSet)\n    labelcounts\u003d{}\n    for i in range(m):\n        label\u003ddataSet[i][-1]\n        labelcounts[label]\u003dlabelcounts.get(label,0)+1\n    entropy\u003d0.0\n    for counts in labelcounts.values():\n        prob\u003dcounts/m\n        entropy-\u003dprob*np.log2(prob)\n    return entropy\n\ndef splitDataSet(dataSet, axis, value):\n    retdataSet\u003d[]\n    for data in dataSet:\n        if data[axis]\u003d\u003dvalue:\n            subFeatures\u003ddata[:axis]\n            subFeatures.extend(data[axis+1:])\n            retdataSet.append(subFeatures)\n    return retdataSet\n\ndef chooseBestFeatureToSplit(dataSet):\n    feature_nums\u003dlen(dataSet[0])-1\n    baseEntropy\u003dcalc_entropy(dataSet)\n    best_infor_gain\u003d0.0\n    best_feature\u003d-1\n    for i in range(feature_nums):\n        feature_list\u003d[example[i] for example in dataSet]\n        unique_value\u003dset(feature_list)\n        new_entropy\u003d0.0\n        for value in unique_value:\n            subDataSet\u003dsplitDataSet(dataSet,i,value)\n            prob\u003dlen(subDataSet)/len(dataSet)\n            new_entropy+\u003dprob*calc_entropy(subDataSet)\n        infor_gain\u003dbaseEntropy-new_entropy\n        if infor_gain\u003ebest_infor_gain:\n            best_infor_gain\u003dinfor_gain\n            best_feature\u003di\n    return best_feature\n\n\ndef majorityCnt(classList):\n    m\u003dlen(classList)\n    class_nums\u003d{}\n    for i in range(m):\n        label\u003dclassList[i]\n        class_nums[label]\u003dclass_nums.get(label,0)+1\n    sorted_class_nums\u003dsorted(class_nums.items(),key\u003dlambda x:x[1],reverse\u003dTrue)\n    return sorted_class_nums[0][0]\n\n#\u003d\u003dID3 algorithm\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\ndef createTree(dataSet,labels):\n    label_list\u003d[example[-1] for example in dataSet]\n    if label_list.count(label_list[0])\u003d\u003dlen(label_list):\n        return label_list[0]\n    if len(dataSet[0])\u003d\u003d1:\n        return majorityCnt(label_list)\n    best_feature\u003dchooseBestFeatureToSplit(dataSet)\n    best_label\u003dlabels[best_feature]\n    my_tree\u003d{best_label:{}}\n    feature_value\u003d[example[best_feature] for example in dataSet]\n    unique_value\u003dset(feature_value)\n    for value in unique_value:\n        sublabels\u003dlabels[0:best_feature]\n        sublabels.extend(labels[best_feature+1:])\n        my_tree[best_label][value]\u003dcreateTree(splitDataSet(dataSet, \\\n               best_feature, value),sublabels)\n    return my_tree\n\nlabels\u003d[]\nfor label in data.columns[1:][:-1]:\n    labels.append(label)\ndata[\u0027density\u0027]\u003ddata[\u0027density\u0027].apply(str)\ndata[\u0027sugar_ratio\u0027]\u003ddata[\u0027sugar_ratio\u0027].apply(str)\nretdata\u003ddata.iloc[:,1:]\ndataSet\u003dretdata.values.tolist()\n\nentropy\u003dcalc_entropy(dataSet)\nentropy\n\n#选择最佳特征\nbest_feature\u003dchooseBestFeatureToSplit(dataSet)\nbest_feature\n\nmy_tree\u003dcreateTree(dataSet,labels)\nmy_tree",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}