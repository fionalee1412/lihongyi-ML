{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "@TOC(线性回归任务一)\n# 学习视频内容：\n\n观看李宏毅课程内容：P1、P2。\n\n视频连接：https://www.bilibili.com/video/av35932863?from\u003dsearch\u0026seid\u003d2134843831238226258\n\n# 学习打卡任务内容：\n\n## 了解什么是Machine learning\n\n类似于泛函，通过输入数据找到一个合适的函数。\n\n## 学习中心极限定理，学习正态分布，学习最大似然估计\n\n中心极限定理：在适当的条件下，大量相互独立随机变量的均值经适当标准化后依分布收敛于正态分布。\n\n棣莫佛－拉普拉斯定理：参数为n, p的二项分布以np为均值、np(1-p)为方差的正态分布为极限。\n\n林德伯格－列维定理：独立同分布且数学期望和方差有限的随机变量序列的标准化和以标准正态分布为极限\n\n正态分布：若随机变量X服从一个位置参数为μ，尺度参数为σ的正态分布，记为X~N(μ，σ2)，则其概率密度函数为\n\n$f(x)\u003d\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-u)^2}{2\\sigma^2}}$\n\n，正态分布的数学期望值或期望值μ等于位置参数，决定了分布的位置；其方差σ^2的开平方或标准差σ等于尺度参数，决定了分布的幅度。常说的标准正态分布是指μ为0，σ为1的正态分布。\n\n最大似然估计：通过事物发生的结果构建似然函数，取令似然函数有最大值的参数值，从而得到当前结果对应的最有可能的先决条件。\n\n### 推导回归Loss function\n\nlogistic/sigmod 函数\n\n$z\u003d\\theta^Tx$\n\n$g(z)\u003d\\frac{1}{1+e^{-z}}$\n\n$g\u0027(z)\u003d-\\frac{e^{-z}}{(1+e^{-z})^2}\u003d\\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}}\u003d\\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}})\u003dg(z)(1-g(z))$\n\n$P(y\u003d1|x;\\theta)\u003dg_\\theta(z)$\n\n$P(y\u003d0|x;\\theta)\u003d1-g_\\theta(z)$\n\n$P(y|x;\\theta)\u003d(g_\\theta(z))^{y}(1-g_\\theta(z))^{(1-y)}$\n\n回归参数估计\n\n$L(\\theta)\u003d\\prod_{i\u003d1}^{m}(g_\\theta(z))^{y^{(i)}}(1-g_\\theta(z))^{(1-y^{(i)})} $\n\n对数似然函数\n\n$l(\\theta)\u003dlog(L(\\theta))\u003d\\sum_{i\u003d1}^{m} [{y^{(i)}}log(g_\\theta(z))+ {(1-y^{(i)})}log(1-g_\\theta(z))]\u003d\\sum_{i\u003d1}^{m} [{y^{(i)}}log(g(\\theta^Tx))+ {(1-y^{(i)})}log(1-g(\\theta^Tx))]$\n\n\n$\\frac{\\partial}{\\partial{\\theta_{j}}} l(\\theta)\u003d({y^{(i)}} \\frac{1}{(g(\\theta^Tx)}- {(1-y^{(i)})}\\frac{1}{(1-g(\\theta^Tx)})\\frac{\\partial}{\\partial \\theta_{j}}g(\\theta^Tx)$\n\n$\u003d({y} \\frac{1}{(g(\\theta^Tx)}- {(1-y)}\\frac{1}{(1-g(\\theta^Tx)})g(\\theta^Tx)(1-g(\\theta^Tx))\\frac{\\partial}{\\partial \\theta_{j}}\\theta^Tx$\n\n$\u003d({y}(1-g(\\theta^Tx))- {(1-y)}g(\\theta^Tx))\\frac{\\partial}{\\partial \\theta_{j}}\\theta^Tx$\n\n\n$\u003d({y}- g(\\theta^Tx))x_j\u003d({y}- h_\\theta(x))x_j$\n\n\n梯度下降法，参数迭代\n\n$\\theta_j :\u003d\\theta_j+\\alpha({y^{(i)}}- h_\\theta(x^{(i)}))x_j^{(i)}$\n\n### 学习损失函数与凸函数之间的关系\n\n- 损失函数：在监督学习中，损失函数刻画了模型和训练样本的匹配程度，即定义了模型的评估指标.\n- 凸函数的几何解释是：函数图像上的任意两点确定的弦在图像的上方；\n- 根据最优化理论，任何最大化问题统一转化为最小问题，任何凹函数统一转化为凸函数，因此将最大化似然函数取反转化为最小优化函数，并将其取反的函数称之为损失函数.\n\n\n### 了解全局最优和局部最优\n\n- 一个函数可能有多个局部最优值，但只会有一个全局最优值\n\n## 学习导数，泰勒展开\n\n- 导数：一个函数在某一点的导数描述了这个函数在这一点附近的变化率。\n- 泰勒公式：泰勒公式是一个函数在某点的信息描述其附近取值的公式，局部有效性。\n\n基本形式：\n\n$f(x)\u003d\\sum_{n\u003d0}^{\\infty}\\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n$\n\n一阶泰勒展开\n\n$f(x) \\approx f(x_0)+f\u0027(x_0)(x-x_0)$ \n\n二阶泰勒展开\n\n$f(x) \\approx f(x_0)+f\u0027(x_0)(x-x_0)+\\frac{f\u0027\u0027(x_0)}{2}(x-x_0)^2$ \n\n### 推导梯度下降公式\n- 梯度下降算法\n\n$J(\\theta)\u003d\\frac{1}{2}\\sum_{i\u003d1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2$\n\n- 初始化$\\theta$\n\n- 源着负梯度迭代，跟新后的$\\theta$使得$J(\\theta)$的值更小\n\n![avatar](https://raw.githubusercontent.com/xumajie/datawhale/master/lihongyi-ML/img/task01_gradient_descent.png)\n\n- 学习率、步长：$\\alpha$\n\n- 梯度方向\n\n$\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\u003d\\frac{\\partial}{\\partial\\theta_j }\\frac{1}{2}(h_\\theta(x)-y)^2$\n\n$\u003d\\frac{1}{2}(h_\\theta(x)-y)^2 \\frac{\\partial}{\\partial\\theta_j }$\n\n$\u003d(h_\\theta(x)-y)x_j$\n\n- 批量梯度下降算法\n\n![avatar](https://raw.githubusercontent.com/xumajie/datawhale/master/lihongyi-ML/img/task01_gradient_descent_thus.png)\n        \n- 批量梯度下降图示\n\n![avatar](https://raw.githubusercontent.com/xumajie/datawhale/master/lihongyi-ML/img/task01_gradient_descent_thus_02.png)\n\n- 随机梯度下降算法\n\n![avatar](https://raw.githubusercontent.com/xumajie/datawhale/master/lihongyi-ML/img/task01_stochastic_gradient_descent.png)\n  \n\n\n\n### 写出梯度下降的代码\n\n![avatar](https://raw.githubusercontent.com/xumajie/datawhale/master/lihongyi-ML/img/task01_gradient_descent_learning_rate.png)\n\n## 学习L2-Norm，L1-Norm，L0-Norm\n\n* 正则化是一个通用的算法和思想，所有会产生过拟合现象的算法都可以使用正则化来避免过拟合\n- L0范数是指向量中非0的元素的个数\n- L1范数是指向量中各个元素绝对值之和，又称曼哈顿距离\n- L2范数: ||W||2。就是欧几里德距离\n\n### 推导正则化公式\n\n$J(\\theta)\u003d\\frac{1}{2m}[\\sum_{i\u003d1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2+\\lambda\\sum_{j\u003d0}^n(\\theta_j)^2]$\n\n注意是j从1开始的。对其求偏导后得到\n\n$\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\u003d\\frac{1}{2}[(h_\\theta(x)-y)^2+\\lambda（\\theta_j)^2 ]\\frac{\\partial}{\\partial\\theta_j }$\n\n$\u003d(h_\\theta(x)-y)x_{ij}+\\lambda\\theta_j$\n\n$\u003d\\theta_j-\\alpha \\cdot\\frac{\\partial J(\\theta)}{\\partial \\theta_j}$\n\n$\u003d(1-\\alpha\\lambda)\\theta_j-\\alpha \\cdot(h_\\theta(x)-y)x_{ij}$\n\n注意当j为0的时候，可以认为$\\lambda$的值为0。可以看出没有正则化时，$\\theta_j$系数的权重为1，而现在明显\n\n$(1-\\alpha\\lambda) \u003c1 $\n\n也就是说权值进行了衰减。\n\n### 说明为什么用L1-Norm代替L0-Norm\n\n- L1-norm 相当于L0-norm，使得特征较多为0，即特征稀疏化，而L0-norm难于优化，因此常用L1-norm代替L0-norm，方便特征提取.\n\n### 学习为什么只对w/Θ做限制，不对b做限制\n\n因为b不影响函数的平滑程度，只影响整体平移，因此b对模型的泛化能力没有影响，但是w的权重分配最终会影响到模型的泛化能力，是模型有过拟合和前拟合的可能。\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "1.866666666666667\n1.991111111111111\n1.9994074074074075\n1.9999604938271605\n1.999997366255144\n1.9999998244170096\n1.9999999882944675\n1.9999999992196311\n1.9999999999479754\n1.9999999999965319\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\n\"\"\"梯度下降的代码\"\"\"\n\nimport numpy as np\nx \u003d np.array([1,2,3])\ny \u003d np.array([2,4,6])\n\nepoches \u003d 10\nlr \u003d 0.1\nw \u003d 0\ncost\u003d[]\n\nfor epoch in range(epoches):\n    yhat \u003d x*w\n    loss \u003d np.average((yhat-y)**2)\n    cost.append(loss)\n    dw \u003d -2*(y-yhat)@ x.T/(x.shape[0])\n    w\u003dw-lr*dw\n    print(w)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}