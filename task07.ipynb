{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "# 决策树分类\n## 熵\n\u003e 熵最早来原于物理学. 德国物理学家鲁道夫·克劳修斯首次提出熵的概念，用来表示任何一种能量在空间中分布的均匀程度，能量分布得越均匀，熵就越大\n\n\u003e从微观看，熵就表现了这个系统所处状态的不确定性程度。香农，描述一个信息系统的时候就借用了熵的概念，这里熵表示的是这个信息系统的平均信息量(平均不确定程度)\n### 数学定义\n\n一个随机变量取值是离散的，取第i种情况的概率是$P_i$,比如扔硬币，正面朝上的概率是p，方面朝上概率是1-p\n\n定义这个分布的熵是：$H(p)\u003d-\\displaystyle\\sum_ip_i\\ln p_i$\n\n回到硬币的栗子\n\n$H(p)\u003d-p\\ln p-(1-p)\\ln (1-p)$\n\n$\\frac{dH(p)}{dp}\u003d\\ln \\frac{1-p}{p}\u003d0\\Rightarrow p\u003d\\frac{1}{2}$\n\n## 联合概率(Joint distribution)\n### 定义\n\n对于$(X,Y)二维随机变量，全部可能可以取得的值是有限对或则是无限对,则称$(X,Y)$是离散型随机变量.\n\n设二维离散型随机变量$(X,Y)$所有可能取到的值为$(x_i,y_j),i,j\u003d1,2...,$记$P(X\u003dx_i,Y\u003dy_i)\u003dp_{i,j},i,j\u003d1,2...,$则有概率的定义有\n\n$P_{i,j} \\geq 0; \\displaystyle \\sum_{i\u003d1}^{\\infty}\\sum_{j\u003d1}^{\\infty}p_{i,j}\u003d1$\n\n我们称$(x_i,y_j),i,j\u003d1,2...,$记$P(X\u003dx_i,Y\u003dy_i)\u003dp_{i,j},i,j\u003d1,2...,$是二维离散随机变量$(X,Y)$的分布律，或称随机变量X和Y的联合分布律\n\n计算方法：\n\n$P\\{X\u003di,Y\u003dj\\}\u003dP\\{Y\u003dj|X\u003di\\}P\\{X\u003di\\},i\u003d1,2,3...,j \\leq i$\n\n## 边缘概率(marginal distribution)\n### 定义\n\n- 提取出任意一个单一随机变量的分布，也就是所谓的边缘分布(marginal distribution)。\n\n对于离散型随机变量，可得X的边缘分布函数：\n\n$F_x(x)\u003dF(x,\\infty)\u003d\\displaystyle \\sum_{x_{x_i} \\leq x}\\displaystyle \\sum_{j\u003d x}^{\\infty} p_{i,j}$\n\n由此，我们得出X的分布律为：\n\n$P\\{X\u003dx_i\\}\u003d\\displaystyle \\sum_{j\u003d1}^{\\infty} p_{i,j},i\u003d1,2,...$\n\n记：$p_i\u003d\\displaystyle \\sum_{j\u003d1}^{\\infty}p_{ij}\u003dP\\{X\u003dx_i\\},i\u003d1,2,...$\n\n则称$p_i$为$(X,Y)$关于X的边缘分布律\n\n## 条件概率(propotal distribution)\n\n### 定义\n\n设A，B是两个事件，且 $P\\left( A \\right)\u003e0$, 称\n\n$P(B|A)\u003d\\frac{P(AB)}{P(A)}$\n\n为在事件A发生的条件下事件B发生的条件概率。\n\n## 推导条件熵定义公式\n### 定义\n\n代表在已知一个变量发生的条件下，另一个变量发生所新增的不确定性；公式为：\n\n$H(X|Y)\u003dH(X,Y)-H(X)\u003d-\\displaystyle \\sum_{x \\in X,y \\in Y}^{x,y}p(x,y)\\ln p(x|y)$这里的X,Y是随机变量的取值集合，\n从上图中理解就是两个大圆去掉其中一个大圆所剩下的面积。\n\n最后定义互信息，其中的一个定义就是在已知X发生的条件下，Y不确定性减少的程度，这个定义在ID3算法中也叫信息增益，计算公式可以理解为：\n\n$I(X,Y)\u003dH(Y)-H(Y|X)\u003dH(Y)+H(X)-H(X,Y)$\n\n$\u003d\\displaystyle \\sum_{x \\in X,y \\in Y} p(x,y)\\ln \\frac{p(x,y)}{p(x)p(y)}$这里的X,Y是随机变量的取值集合，从上图理解就是两个大圆相交的面积，所以互信息是对偶的。\n\n## 联合熵\n### 定义\n![avatar](https://raw.githubusercontent.com/xmj-datawhale/lihongyi-ML/master/img/task6_joint_entropy1.png)\n\n代表X,Y同时发生的不确定性；公式为：$H(X,Y)\u003d-\\displaystyle \\sum_{x \\in X,y \\in Y}^{x,y}p(x,y)\\ln p(x,y)$这里的X,Y是随机变量的取值集合，从上图理解就是两个大圆的面积\n## 相对熵\n## 互信息\n## 交叉熵 loss function\n\nhttps://www.cnblogs.com/arachis/p/entropy.html\n## 决策树\n## CART树"
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}